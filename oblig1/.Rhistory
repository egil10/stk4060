df <- df |>
group_by(.data[[group_col]]) |>
summarise(value = mean(.data[[value_col]], na.rm = TRUE)) |>
arrange(desc(value))
json_obj <- list(
title = title,
description = description,
unit = unit,
source = source,
data = setNames(
lapply(seq_len(nrow(df)), function(i) {
list(value = df$value[i], unit = unit)
}),
df[[group_col]]
)
)
dir.create("output", showWarnings = FALSE)
write(toJSON(json_obj, pretty = TRUE, auto_unbox = TRUE),
paste0("output/", file_name, ".json"))
}
# ---------------------------
# 1. Load the Excel file
# ---------------------------
file <- "data/undesa_pd_2019_wmd_marital_status.xlsx"
sheets <- excel_sheets(file)
print(sheets)  # Check available sheets
# Example sheet loading (we’ll use pattern matching)
read_and_clean <- function(sheet) {
df <- read_excel(file, sheet = sheet, skip = 0)
df <- clean_names(df)
df
}
marital_status <- read_and_clean("MARITAL_STATUS_BY_AGE")
currently_married <- read_and_clean("CURRENTLY MARRIED")
ever_married <- read_and_clean("EVER_MARRIED")
smam <- read_and_clean("SMAM")
# ---------------------------
# 2. Identify numeric column automatically
# ---------------------------
get_numeric_col <- function(df) {
# Find likely "DataValue" column even if name changed (case insensitive)
cand <- names(df)[str_detect(names(df), regex("data.?value", ignore_case = TRUE))]
if (length(cand) == 0) stop("No DataValue-like column found!")
cand[1]
}
value_col_marital <- get_numeric_col(marital_status)
library(readxl)
library(dplyr)
library(stringr)
library(jsonlite)
library(tidyr)
# ---------------------------
# Helper functions
# ---------------------------
clean_names <- function(df) {
names(df) <- names(df) |>
str_replace_all("[^A-Za-z0-9]+", "_") |>
str_trim() |>
str_remove("_$") |>
str_to_title()
df
}
clean_num <- function(x) {
as.numeric(str_replace(x, ",", "."))
}
make_json <- function(df, title, description, unit, source, value_col, group_col, file_name) {
df <- df |>
group_by(.data[[group_col]]) |>
summarise(value = mean(.data[[value_col]], na.rm = TRUE)) |>
arrange(desc(value))
json_obj <- list(
title = title,
description = description,
unit = unit,
source = source,
data = setNames(
lapply(seq_len(nrow(df)), function(i) {
list(value = df$value[i], unit = unit)
}),
df[[group_col]]
)
)
dir.create("output", showWarnings = FALSE)
write(toJSON(json_obj, pretty = TRUE, auto_unbox = TRUE),
paste0("output/", file_name, ".json"))
}
# ---------------------------
# Smarter sheet reader
# ---------------------------
read_and_clean <- function(file, sheet) {
# Read first few rows to detect where data starts
temp <- read_excel(file, sheet = sheet, n_max = 5)
first_header_row <- which(apply(temp, 1, function(r) any(str_detect(r, "Country", ignore_case = TRUE))))[1]
if (is.na(first_header_row)) first_header_row <- 3  # default fallback
df <- read_excel(file, sheet = sheet, skip = first_header_row - 1)
df <- clean_names(df)
df
}
# ---------------------------
# Load file and sheets
# ---------------------------
file <- "data/undesa_pd_2019_wmd_marital_status.xlsx"
marital_status <- read_and_clean(file, "MARITAL_STATUS_BY_AGE")
# Required packages
library(readr)
library(dplyr)
library(jsonlite)
# === 1. Read CSV file ===
# Replace with your actual file path
df <- read_csv("data/oecds.csv", show_col_types = FALSE)
# === 2. Clean and select relevant columns ===
# The dataset includes many meta columns — we keep the essentials
df_clean <- df %>%
select(
REF_AREA,           # country code
`Reference area`,   # country name
TIME_PERIOD,        # year
OBS_VALUE,          # value (hours)
`Unit of measure`
) %>%
rename(
Country = `Reference area`,
Year = TIME_PERIOD,
Value = OBS_VALUE,
Unit = `Unit of measure`
) %>%
filter(!is.na(Value))
# === 3. Create one JSON file per country ===
# Create an output folder
dir.create("json_countries", showWarnings = FALSE)
# Loop over countries
unique_countries <- unique(df_clean$Country)
for (country in unique_countries) {
country_data <- df_clean %>% filter(Country == country)
latest <- country_data %>% filter(Year == max(Year, na.rm = TRUE))
json_output <- list(
title = "Average Annual Hours Actually Worked Per Worker",
description = paste0(
"Average annual hours actually worked per worker in ",
country, ", latest available year (", latest$Year[1], ")"
),
unit = unique(country_data$Unit)[1],
source = "https://stats.oecd.org/Index.aspx?DataSetCode=SAE",
data = list(
country = list(
value = as.numeric(latest$Value[1]),
unit = unique(country_data$Unit)[1]
)
)
)
# Write JSON file
json_file_name <- paste0("json_countries/", gsub(" ", "_", country), ".json")
write_json(json_output, json_file_name, pretty = TRUE, auto_unbox = TRUE)
}
cat("✅ JSON files generated for", length(unique_countries), "countries in /json_countries\n")
# === Required packages ===
library(readr)
library(dplyr)
library(jsonlite)
# === 1. Read data ===
# Replace with your actual file path
df <- read_csv("data/oecds.csv", show_col_types = FALSE)
# === 2. Keep relevant columns ===
df_clean <- df %>%
select(`Reference area`, REF_AREA, TIME_PERIOD, OBS_VALUE, `Unit of measure`) %>%
rename(
Country = `Reference area`,
Year = TIME_PERIOD,
Value = OBS_VALUE,
Unit = `Unit of measure`
) %>%
filter(!is.na(Value))
# === 3. Keep only latest year per country ===
latest_data <- df_clean %>%
group_by(Country) %>%
filter(Year == max(Year, na.rm = TRUE)) %>%
ungroup()
# === 4. Build JSON structure ===
data_list <- setNames(
lapply(seq_len(nrow(latest_data)), function(i) {
list(
value = as.numeric(latest_data$Value[i]),
unit = latest_data$Unit[i]
)
}),
latest_data$Country
)
json_output <- list(
title = "Average Annual Hours Actually Worked Per Worker",
description = paste0(
"Latest available year of average annual hours actually worked per worker, OECD dataset (as of ",
max(latest_data$Year, na.rm = TRUE),
")"
),
unit = unique(latest_data$Unit)[1],
source = "https://stats.oecd.org/Index.aspx?DataSetCode=SAE",
data = data_list
)
# === 5. Write JSON file ===
write_json(json_output, "oecd_hours_latest.json", pretty = TRUE, auto_unbox = TRUE)
cat("✅ JSON file 'oecd_hours_latest.json' generated successfully!\n")
# === Required packages ===
library(readr)
library(dplyr)
library(jsonlite)
# === 1. Read data ===
# Replace with your actual file path
df <- read_csv("data/oecds.csv", show_col_types = FALSE)
# === 2. Keep relevant columns ===
df_clean <- df %>%
select(`Reference area`, REF_AREA, TIME_PERIOD, OBS_VALUE, `Unit of measure`) %>%
rename(
Country = `Reference area`,
Year = TIME_PERIOD,
Value = OBS_VALUE,
Unit = `Unit of measure`
) %>%
filter(!is.na(Value))
# === 3. Keep only latest year per country ===
latest_data <- df_clean %>%
group_by(Country) %>%
filter(Year == max(Year, na.rm = TRUE)) %>%
ungroup()
# === 4. Build JSON structure ===
data_list <- setNames(
lapply(seq_len(nrow(latest_data)), function(i) {
list(
value = as.numeric(latest_data$Value[i]),
unit = latest_data$Unit[i]
)
}),
latest_data$Country
)
json_output <- list(
title = "Average Annual Hours Actually Worked Per Worker",
description = paste0(
"Latest available year of average annual hours actually worked per worker, OECD dataset (as of ",
max(latest_data$Year, na.rm = TRUE),
")"
),
unit = unique(latest_data$Unit)[1],
source = "https://stats.oecd.org/Index.aspx?DataSetCode=SAE",
data = data_list
)
# === 5. Write JSON file ===
write_json(json_output, "oecd_hours_latest.json", pretty = TRUE, auto_unbox = TRUE)
cat("✅ JSON file 'oecd_hours_latest.json' generated successfully!\n")
# === Load libraries ===
library(jsonlite)
library(dplyr)
library(purrr)
# === 1. Read Nobel JSON ===
# If saved locally:
# nobel <- fromJSON("nobel_laureates.json", flatten = TRUE)
# Or directly from the API:
nobel <- fromJSON("https://api.nobelprize.org/2.1/laureates?format=json", flatten = TRUE)
laureates <- nobel$laureates
# === 2. Extract relevant fields ===
# Some laureates may have missing birth info or multiple prizes
df <- map_dfr(laureates, function(x) {
tibble(
id = x$id %||% NA_character_,
name = x$fullName.en %||% x$knownName.en %||% NA_character_,
country = x$birth.place.country.en %||% NA_character_,
city = x$birth.place.city.en %||% NA_character_,
birth_year = substr(x$birth.date %||% NA_character_, 1, 4),
gender = x$gender %||% NA_character_,
prize_year = if (!is.null(x$nobelPrizes[[1]]$awardYear)) x$nobelPrizes[[1]]$awardYear else NA_character_,
category = if (!is.null(x$nobelPrizes[[1]]$category.en)) x$nobelPrizes[[1]]$category.en else NA_character_,
motivation = if (!is.null(x$nobelPrizes[[1]]$motivation.en)) x$nobelPrizes[[1]]$motivation.en else NA_character_,
institution = if (!is.null(x$nobelPrizes[[1]]$affiliations[[1]]$name.en)) x$nobelPrizes[[1]]$affiliations[[1]]$name.en else NA_character_,
wikipedia = x$wikipedia.english %||% NA_character_,
wikidata = x$wikidata.url %||% NA_character_
)
})
"USA": {
"name": "Claudia Goldin",
"USA": {
"name": "Claudia Goldin",
# Load packages
library(jsonlite)
library(dplyr)
library(purrr)
# === 1. Read local Nobel laureates data ===
nobel <- fromJSON("data/laureates.json")
laureates <- nobel$laureates
# === 2. Helper for safe extraction ===
safe_extract <- function(x, path, default = NA_character_) {
tryCatch({
val <- purrr::pluck(x, !!!path)
if (is.null(val) || length(val) == 0) default else as.character(val)
}, error = function(e) default)
}
# === 3. Extract key fields per laureate ===
df <- map_dfr(laureates, function(x) {
tibble(
id = safe_extract(x, c("id")),
name = coalesce(
safe_extract(x, c("fullName", "en")),
safe_extract(x, c("knownName", "en"))
),
country = safe_extract(x, c("birth", "place", "country", "en")),
birth_year = substr(safe_extract(x, c("birth", "date")), 1, 4),
gender = safe_extract(x, c("gender")),
prize_year = safe_extract(x, c("nobelPrizes", 1, "awardYear")),
category = safe_extract(x, c("nobelPrizes", 1, "category", "en")),
motivation = safe_extract(x, c("nobelPrizes", 1, "motivation", "en")),
institution = safe_extract(x, c("nobelPrizes", 1, "affiliations", 1, "name", "en")),
wikipedia = safe_extract(x, c("wikipedia", "english")),
wikidata = safe_extract(x, c("wikidata", "url"))
)
})
# === 4. Keep the most recent laureate per country ===
df_clean <- df %>%
filter(!is.na(country) & !is.na(name)) %>%
mutate(prize_year = suppressWarnings(as.numeric(prize_year))) %>%
group_by(country) %>%
arrange(desc(prize_year)) %>%
slice_head(n = 1) %>%
ungroup()
# === 5. Build JSON ===
data_list <- setNames(
lapply(seq_len(nrow(df_clean)), function(i) {
list(
name = df_clean$name[i],
prize_year = df_clean$prize_year[i],
category = df_clean$category[i],
motivation = df_clean$motivation[i],
institution = df_clean$institution[i],
wikipedia = df_clean$wikipedia[i],
wikidata = df_clean$wikidata[i]
)
}),
df_clean$country
)
json_output <- list(
title = "Latest Nobel Prize Laureates by Country",
description = "Each country's most recent Nobel laureate, with award year, category, motivation, and institution.",
source = "https://api.nobelprize.org/2.1/laureates",
data = data_list
)
# === 6. Write to file ===
dir.create("output", showWarnings = FALSE)
write_json(json_output, "output/nobel_latest_by_country.json", pretty = TRUE, auto_unbox = TRUE)
cat("✅ JSON file saved to output/nobel_latest_by_country.json\n")
# Load libraries
library(jsonlite)
library(dplyr)
library(purrr)
# === 1. Read local Nobel laureates file ===
nobel <- fromJSON("data/laureates.json")
laureates <- nobel$laureates
# === 2. Safely extract country name ===
safe_extract <- function(x, path, default = NA_character_) {
tryCatch({
val <- purrr::pluck(x, !!!path)
if (is.null(val) || length(val) == 0) default else as.character(val)
}, error = function(e) default)
}
# === 3. Build dataframe of countries ===
df <- map_dfr(laureates, function(x) {
tibble(
country = safe_extract(x, c("birth", "place", "country", "en"))
)
}) %>%
filter(!is.na(country)) %>%
group_by(country) %>%
summarise(value = n(), .groups = "drop")
# === 4. Build JSON ===
data_list <- setNames(
lapply(seq_len(nrow(df)), function(i) {
list(
value = df$value[i],
unit = "laureates"
)
}),
df$country
)
json_output <- list(
title = "Number of Nobel Laureates by Country",
description = "Total count of Nobel Prize laureates by country of birth (all years)",
unit = "laureates",
source = "https://api.nobelprize.org/2.1/laureates",
data = data_list
)
# === 5. Write to file ===
dir.create("output", showWarnings = FALSE)
write_json(json_output, "output/nobel_laureates_by_country.json", pretty = TRUE, auto_unbox = TRUE)
cat("✅ File saved to output/nobel_laureates_by_country.json\n")
library(jsonlite)
library(dplyr)
library(purrr)
# === 1. Read local Nobel laureates file ===
nobel <- fromJSON("data/laureates.json")
laureates <- nobel$laureates
# === 2. Helper for safe extraction ===
safe_extract <- function(x, paths) {
for (path in paths) {
val <- tryCatch(purrr::pluck(x, !!!path), error = function(e) NULL)
if (!is.null(val) && length(val) > 0 && !is.na(val)) return(as.character(val))
}
return(NA_character_)
}
# === 3. Try multiple possible country paths ===
df <- map_dfr(laureates, function(x) {
tibble(
country = safe_extract(x, list(
c("birth", "place", "country", "en"),
c("birth", "place", "countryNow", "en"),
c("birth", "place", "country", "no"),
c("birth", "place", "country", "se")
))
)
})
# === 4. Count laureates per country ===
df_clean <- df %>%
filter(!is.na(country) & country != "") %>%
group_by(country) %>%
summarise(value = n(), .groups = "drop")
# === 5. Build JSON ===
data_list <- setNames(
lapply(seq_len(nrow(df_clean)), function(i) {
list(
value = df_clean$value[i],
unit = "laureates"
)
}),
df_clean$country
)
json_output <- list(
title = "Number of Nobel Laureates by Country",
description = "Total count of Nobel Prize laureates by country of birth (all years)",
unit = "laureates",
source = "https://api.nobelprize.org/2.1/laureates",
data = data_list
)
# === 6. Write to file ===
dir.create("output", showWarnings = FALSE)
write_json(json_output, "output/nobel_laureates_by_country.json", pretty = TRUE, auto_unbox = TRUE)
cat("✅ Nobel laureates by country saved to output/nobel_laureates_by_country.json\n")
library(tidyverse)
setwd("C:/Users/ofurn/Dokumenter/Github/stk1110/oblig2")
read.table("data/kveite_for.txt",
sep = "",
header = T)
d = read.table("data/kveite_for.txt",
sep = "",
header = T)
d
boxplot(d)
boxplot(d$Vekt)
boxplot(d$Vekt, d$Fortype)
d %>%
ggplot(aes(x = Vekt, y = Fortype)) +
geom_boxplot()
d %>%
ggplot(aes(x = Vekt, y = Fortype)) +
geom_boxplot()
d
d %>%
ggplot(aes(x = Vekt, col = Fortype)) +
geom_boxplot()
d %>%
ggplot(aes(x = Vekt)) +
geom_boxplot()
boxplot(d$Vekt)
boxplot(d$Vekt)
library(tidyverse)
d = read.table("data/snoe_vann.txt")
d
lm(d$V1, d$V2)
d %>% lm(V1~V2)
lm(V1~V2, data = d)
library(tidyverse)
setwd("C:/Users/ofurn/Dokumenter/Github/stk3100/oblig2")
read.csv("data/fertility_data.csv",
header = TRUE)
fertility = read.csv("data/fertility_data.csv",
header = TRUE)
fertility
fertility %>% as_tibble()
source("C:/Users/ofurn/Dokumenter/Github/stk3100/oblig2/1a.R")
library(MASS)
MASS::glm.nb()
clear
library(tidyverse)
setwd("C:/Users/ofurn/Dokumenter/Github/stk-in4300/oblig2")
read.csv("data/qsar_aquatic_toxicity.csv")
read.csv("data/qsar_aquatic_toxicity.csv",
sep = ";")
df = read.csv("data/qsar_aquatic_toxicity.csv",
sep = ";") %>%
library(tidyverse)
df = read.csv("data/qsar_aquatic_toxicity.csv",
sep = ";") %>%
as_tibble()
df
df
df = read.csv("data/qsar_aquatic_toxicity.csv",
sep = ";",
header = F) %>%
as_tibble()
df
install.packages("mlbench")
library(tidyverse)
library(mlbench)
data("PimaIndiansDiabetes")
PimaIndiansDiabetes
PimaIndiansDiabetes %>%
as_tibble()
setwd("C:/Users/ofurn/Dokumenter/Github/stk4060/oblig1")
clear
